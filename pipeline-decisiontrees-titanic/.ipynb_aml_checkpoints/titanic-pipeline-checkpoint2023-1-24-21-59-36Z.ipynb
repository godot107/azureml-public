{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\r\n",
        "\r\n",
        "# Motivation"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\r\n",
        "\r\n",
        "import azureml.core\r\n",
        "from azureml.core import Workspace\r\n",
        "\r\n",
        "from azureml.core import Dataset\r\n",
        "from azureml.data.datapath import DataPath\r\n",
        "\r\n",
        "import os"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677275928619
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to Workspace"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "# Load the workspace from the saved config file\r\n",
        "ws = Workspace.from_config()\r\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Ready to use Azure ML 1.48.0 to work with testerinos\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1677275928936
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "\r\n",
        "default_ds = ws.get_default_datastore()\r\n",
        "\r\n",
        "if 'titanic dataset' not in ws.datasets:\r\n",
        "    Dataset.File.upload_directory(src_dir='data',\r\n",
        "                              target=DataPath(default_ds, 'titanic-data/')\r\n",
        "                              )\r\n",
        "\r\n",
        "    #Create a tabular dataset from the path on the datastore (this may take a short while)\r\n",
        "    tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'titanic-data/titanic.csv'))\r\n",
        "\r\n",
        "    # Register the tabular dataset\r\n",
        "    try:\r\n",
        "        tab_data_set = tab_data_set.register(workspace=ws, \r\n",
        "                                name='titanic dataset',\r\n",
        "                                description='titanic data',\r\n",
        "                                tags = {'format':'CSV'},\r\n",
        "                                create_new_version=True)\r\n",
        "        print('Dataset registered.')\r\n",
        "    except Exception as ex:\r\n",
        "        print(ex)\r\n",
        "else:\r\n",
        "    print('Dataset already registered.')"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677275929521
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "# Create a folder for the pipeline step files\r\n",
        "experiment_folder = 'titanic_pipeline'\r\n",
        "os.makedirs(experiment_folder, exist_ok=True)\r\n",
        "\r\n",
        "print(experiment_folder)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "titanic_pipeline\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677275929695
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/prep_titanic.py\r\n",
        "# Import libraries\r\n",
        "import os\r\n",
        "import argparse\r\n",
        "import pandas as pd\r\n",
        "from azureml.core import Run\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "\r\n",
        "# Get parameters\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument(\"--input-data\", type=str, dest='raw_dataset_id', help='raw dataset')\r\n",
        "parser.add_argument('--prepped-data', type=str, dest='prepped_data', default='prepped_data', help='Folder for results')\r\n",
        "args = parser.parse_args()\r\n",
        "save_folder = args.prepped_data\r\n",
        "\r\n",
        "# Get the experiment run context\r\n",
        "run = Run.get_context()\r\n",
        "\r\n",
        "# load the data (passed as an input dataset)\r\n",
        "print(\"Loading Data...\")\r\n",
        "df = run.input_datasets['raw_data'].to_pandas_dataframe()\r\n",
        "\r\n",
        "# Log raw row count\r\n",
        "row_count = (len(df))\r\n",
        "run.log('raw_rows', row_count)\r\n",
        "\r\n",
        "# remove nulls\r\n",
        "df = df.dropna()\r\n",
        "\r\n",
        "# Normalize the numeric columns\r\n",
        "scaler = MinMaxScaler()\r\n",
        "num_cols = ['Age','Fare']\r\n",
        "#df[num_cols] = scaler.fit_transform(df[num_cols])\r\n",
        "\r\n",
        "df['Sex'] = df['Sex'].replace({'male':1,'female':0})\r\n",
        "\r\n",
        "# Log processed rows\r\n",
        "row_count = (len(df))\r\n",
        "run.log('processed_rows', row_count)\r\n",
        "\r\n",
        "# Save the prepped data\r\n",
        "print(\"Saving Data...\")\r\n",
        "os.makedirs(save_folder, exist_ok=True)\r\n",
        "save_path = os.path.join(save_folder,'data.csv')\r\n",
        "df.to_csv(save_path, index=False, header=True)\r\n",
        "\r\n",
        "# End the run\r\n",
        "run.complete()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting titanic_pipeline/prep_titanic.py\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/train_titanic.py\r\n",
        "# Import libraries\r\n",
        "from azureml.core import Run, Model\r\n",
        "import argparse\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import joblib\r\n",
        "import os\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "from sklearn.metrics import roc_curve\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# Get parameters\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument(\"--training-data\", type=str, dest='training_data', help='training data')\r\n",
        "args = parser.parse_args()\r\n",
        "training_data = args.training_data\r\n",
        "\r\n",
        "# Get the experiment run context\r\n",
        "run = Run.get_context()\r\n",
        "\r\n",
        "# load the prepared data file in the training folder\r\n",
        "print(\"Loading Data...\")\r\n",
        "file_path = os.path.join(training_data,'data.csv')\r\n",
        "titanic = pd.read_csv(file_path)\r\n",
        "\r\n",
        "# Separate features and labels\r\n",
        "X, y = titanic[['Age','Sex','Fare']].values, titanic['Survived'].values\r\n",
        "\r\n",
        "# Split data into training set and test set\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n",
        "\r\n",
        "# Train adecision tree model\r\n",
        "print('Training a decision tree model...')\r\n",
        "model = DecisionTreeClassifier().fit(X_train, y_train)\r\n",
        "\r\n",
        "# calculate accuracy\r\n",
        "y_hat = model.predict(X_test)\r\n",
        "acc = np.average(y_hat == y_test)\r\n",
        "print('Accuracy:', acc)\r\n",
        "run.log('Accuracy', np.float(acc))\r\n",
        "\r\n",
        "# calculate AUC\r\n",
        "y_scores = model.predict_proba(X_test)\r\n",
        "auc = roc_auc_score(y_test,y_scores[:,1])\r\n",
        "print('AUC: ' + str(auc))\r\n",
        "run.log('AUC', np.float(auc))\r\n",
        "\r\n",
        "# plot ROC curve\r\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\r\n",
        "fig = plt.figure(figsize=(6, 4))\r\n",
        "# Plot the diagonal 50% line\r\n",
        "plt.plot([0, 1], [0, 1], 'k--')\r\n",
        "# Plot the FPR and TPR achieved by our model\r\n",
        "plt.plot(fpr, tpr)\r\n",
        "plt.xlabel('False Positive Rate')\r\n",
        "plt.ylabel('True Positive Rate')\r\n",
        "plt.title('ROC Curve')\r\n",
        "run.log_image(name = \"ROC\", plot = fig)\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Save the trained model in the outputs folder\r\n",
        "print(\"Saving model...\")\r\n",
        "os.makedirs('outputs', exist_ok=True)\r\n",
        "model_file = os.path.join('outputs', 'titanic_model.pkl')\r\n",
        "joblib.dump(value=model, filename=model_file)\r\n",
        "\r\n",
        "# Register the model\r\n",
        "print('Registering model...')\r\n",
        "Model.register(workspace=run.experiment.workspace,\r\n",
        "               model_path = model_file,\r\n",
        "               model_name = 'titanic_model',\r\n",
        "               tags={'Training context':'Pipeline'},\r\n",
        "               properties={'AUC': np.float(auc), 'Accuracy': np.float(acc)})\r\n",
        "\r\n",
        "\r\n",
        "run.complete()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting titanic_pipeline/train_titanic.py\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "cluster_name = \"sweetdreams\"\r\n",
        "\r\n",
        "try:\r\n",
        "    # Check for existing compute target\r\n",
        "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
        "    print('Found existing cluster, use it.')\r\n",
        "except ComputeTargetException:\r\n",
        "    # If it doesn't already exist, create it\r\n",
        "    try:\r\n",
        "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\r\n",
        "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
        "        pipeline_cluster.wait_for_completion(show_output=True)\r\n",
        "    except Exception as ex:\r\n",
        "        print(ex)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found existing cluster, use it.\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677275930481
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/experiment_env.yml\r\n",
        "name: experiment_env\r\n",
        "dependencies:\r\n",
        "- python=3.6.2\r\n",
        "- scikit-learn\r\n",
        "- ipykernel\r\n",
        "- matplotlib\r\n",
        "- pandas\r\n",
        "- pip\r\n",
        "- pip:\r\n",
        "  - azureml-defaults\r\n",
        "  - pyarrow"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting titanic_pipeline/experiment_env.yml\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\r\n",
        "from azureml.core.runconfig import RunConfiguration\r\n",
        "\r\n",
        "# Create a Python environment for the experiment (from a .yml file)\r\n",
        "experiment_env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/experiment_env.yml\")\r\n",
        "\r\n",
        "# Register the environment \r\n",
        "experiment_env.register(workspace=ws)\r\n",
        "registered_env = Environment.get(ws, 'experiment_env')\r\n",
        "\r\n",
        "# Create a new runconfig object for the pipeline\r\n",
        "pipeline_run_config = RunConfiguration()\r\n",
        "\r\n",
        "# Use the compute you created above. \r\n",
        "pipeline_run_config.target = pipeline_cluster\r\n",
        "\r\n",
        "# Assign the environment to the run configuration\r\n",
        "pipeline_run_config.environment = registered_env\r\n",
        "\r\n",
        "print (\"Run configuration created.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Run configuration created.\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677275931184
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.data import OutputFileDatasetConfig\r\n",
        "from azureml.pipeline.steps import PythonScriptStep\r\n",
        "\r\n",
        "# Get the training dataset\r\n",
        "titanic_ds = ws.datasets.get(\"titanic dataset\")\r\n",
        "\r\n",
        "# Create an OutputFileDatasetConfig (temporary Data Reference) for data passed from step 1 to step 2\r\n",
        "prepped_data = OutputFileDatasetConfig(\"prepped_data\")\r\n",
        "\r\n",
        "# Step 1, Run the data prep script\r\n",
        "prep_step = PythonScriptStep(name = \"Prepare Data\",\r\n",
        "                                source_directory = experiment_folder,\r\n",
        "                                script_name = \"prep_titanic.py\",\r\n",
        "                                arguments = ['--input-data', titanic_ds.as_named_input('raw_data'),\r\n",
        "                                             '--prepped-data', prepped_data],\r\n",
        "                                compute_target = pipeline_cluster,\r\n",
        "                                runconfig = pipeline_run_config,\r\n",
        "                                allow_reuse = True)\r\n",
        "\r\n",
        "# Step 2, run the training script\r\n",
        "train_step = PythonScriptStep(name = \"Train and Register Model\",\r\n",
        "                                source_directory = experiment_folder,\r\n",
        "                                script_name = \"train_titanic.py\",\r\n",
        "                                arguments = ['--training-data', prepped_data.as_input()],\r\n",
        "                                compute_target = pipeline_cluster,\r\n",
        "                                runconfig = pipeline_run_config,\r\n",
        "                                allow_reuse = True)\r\n",
        "\r\n",
        "print(\"Pipeline steps defined\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline steps defined\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677275931483
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "from azureml.core import Experiment\r\n",
        "from azureml.pipeline.core import Pipeline\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "\r\n",
        "# Construct the pipeline\r\n",
        "pipeline_steps = [prep_step, train_step]\r\n",
        "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\r\n",
        "print(\"Pipeline is built.\")\r\n",
        "\r\n",
        "# Create an experiment and run the pipeline\r\n",
        "experiment = Experiment(workspace=ws, name = 'titanic-pipeline')\r\n",
        "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\r\n",
        "print(\"Pipeline submitted for execution.\")\r\n",
        "RunDetails(pipeline_run).show()\r\n",
        "pipeline_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline is built.\nCreated step Prepare Data [cdee1678][17c00503-0fd2-4b26-820d-5a3f5bfdd725], (This step will run and generate new outputs)\nCreated step Train and Register Model [dff3c476][ad331ddc-fc64-444a-aae0-402eaf514abc], (This step will run and generate new outputs)\nSubmitted PipelineRun d37c4701-e878-476b-84ad-ee3118bc7ebd\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/d37c4701-e878-476b-84ad-ee3118bc7ebd?wsid=/subscriptions/71fa0172-ce90-403c-94a9-14ce1e88f56a/resourcegroups/rg_eastus_44930_1_1677271518341/workspaces/testerinos&tid=82676786-5bc7-43c6-b8f8-b3ee02b0b5f3\nPipeline submitted for execution.\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba6db3cc9a2b499085bd75d7179f9377"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/d37c4701-e878-476b-84ad-ee3118bc7ebd?wsid=/subscriptions/71fa0172-ce90-403c-94a9-14ce1e88f56a/resourcegroups/rg_eastus_44930_1_1677271518341/workspaces/testerinos&tid=82676786-5bc7-43c6-b8f8-b3ee02b0b5f3\", \"run_id\": \"d37c4701-e878-476b-84ad-ee3118bc7ebd\", \"run_properties\": {\"run_id\": \"d37c4701-e878-476b-84ad-ee3118bc7ebd\", \"created_utc\": \"2023-02-24T21:58:53.357286Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{}\", \"azureml.continue_on_step_failure\": \"False\", \"azureml.continue_on_failed_optional_input\": \"True\", \"azureml.pipelineComponent\": \"pipelinerun\"}, \"tags\": {}, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://testerinos3839517853.blob.core.windows.net/azureml/ExperimentRun/dcid.d37c4701-e878-476b-84ad-ee3118bc7ebd/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=SBU2hSlScQfg6rEoUEJZ9tDBqRR8OhFdAXtam7EQ1q4%3D&skoid=e05e5411-329b-4eb9-b14a-b076466588e2&sktid=82676786-5bc7-43c6-b8f8-b3ee02b0b5f3&skt=2023-02-24T21%3A03%3A18Z&ske=2023-02-26T05%3A13%3A18Z&sks=b&skv=2019-07-07&st=2023-02-24T21%3A48%3A57Z&se=2023-02-25T05%3A58%3A57Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://testerinos3839517853.blob.core.windows.net/azureml/ExperimentRun/dcid.d37c4701-e878-476b-84ad-ee3118bc7ebd/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=gkuTJVoQiriZiBaDx0wPpvRuO2Bke9qDC9IMxBW49xM%3D&skoid=e05e5411-329b-4eb9-b14a-b076466588e2&sktid=82676786-5bc7-43c6-b8f8-b3ee02b0b5f3&skt=2023-02-24T21%3A03%3A18Z&ske=2023-02-26T05%3A13%3A18Z&sks=b&skv=2019-07-07&st=2023-02-24T21%3A48%3A57Z&se=2023-02-25T05%3A58%3A57Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://testerinos3839517853.blob.core.windows.net/azureml/ExperimentRun/dcid.d37c4701-e878-476b-84ad-ee3118bc7ebd/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=mtXfNjrDlv%2FBZVniVVgWB%2FnSVI1n83YiwF%2BfEkssU9E%3D&skoid=e05e5411-329b-4eb9-b14a-b076466588e2&sktid=82676786-5bc7-43c6-b8f8-b3ee02b0b5f3&skt=2023-02-24T21%3A03%3A18Z&ske=2023-02-26T05%3A13%3A18Z&sks=b&skv=2019-07-07&st=2023-02-24T21%3A48%3A57Z&se=2023-02-25T05%3A58%3A57Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:00:26\", \"run_number\": \"1677275933\", \"run_queued_details\": {\"status\": \"Running\", \"details\": null}}, \"child_runs\": [{\"run_id\": \"06396784-ed66-4d07-b53e-d9e9c77ab71c\", \"name\": \"Prepare Data\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"2023-02-24T21:58:56.464321Z\", \"end_time\": \"\", \"duration\": \"0:00:24\", \"run_number\": 1677275936, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2023-02-24T21:58:56.464321Z\", \"is_reused\": \"\"}, {\"run_id\": \"\", \"name\": \"Train and Register Model\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2023-02-24 21:58:54Z] Submitting 1 runs, first five are: cdee1678:06396784-ed66-4d07-b53e-d9e9c77ab71c\\n\", \"graph\": {\"datasource_nodes\": {\"673eb5f1\": {\"node_id\": \"673eb5f1\", \"name\": \"titanic dataset\"}}, \"module_nodes\": {\"cdee1678\": {\"node_id\": \"cdee1678\", \"name\": \"Prepare Data\", \"status\": \"NotStarted\", \"_is_reused\": false, \"run_id\": \"06396784-ed66-4d07-b53e-d9e9c77ab71c\"}, \"dff3c476\": {\"node_id\": \"dff3c476\", \"name\": \"Train and Register Model\", \"status\": \"NotStarted\"}}, \"edges\": [{\"source_node_id\": \"673eb5f1\", \"source_node_name\": \"titanic dataset\", \"source_name\": \"data\", \"target_name\": \"raw_data\", \"dst_node_id\": \"cdee1678\", \"dst_node_name\": \"Prepare Data\"}, {\"source_node_id\": \"cdee1678\", \"source_node_name\": \"Prepare Data\", \"source_name\": \"prepped_data\", \"target_name\": \"input_96fdb641\", \"dst_node_id\": \"dff3c476\", \"dst_node_name\": \"Train and Register Model\"}], \"child_runs\": [{\"run_id\": \"06396784-ed66-4d07-b53e-d9e9c77ab71c\", \"name\": \"Prepare Data\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"2023-02-24T21:58:56.464321Z\", \"end_time\": \"\", \"duration\": \"0:00:24\", \"run_number\": 1677275936, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2023-02-24T21:58:56.464321Z\", \"is_reused\": \"\"}, {\"run_id\": \"\", \"name\": \"Train and Register Model\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.48.0\"}, \"loading\": false}"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "PipelineRunId: d37c4701-e878-476b-84ad-ee3118bc7ebd\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/d37c4701-e878-476b-84ad-ee3118bc7ebd?wsid=/subscriptions/71fa0172-ce90-403c-94a9-14ce1e88f56a/resourcegroups/rg_eastus_44930_1_1677271518341/workspaces/testerinos&tid=82676786-5bc7-43c6-b8f8-b3ee02b0b5f3\nPipelineRun Status: Running\n\n\nStepRunId: 06396784-ed66-4d07-b53e-d9e9c77ab71c\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/06396784-ed66-4d07-b53e-d9e9c77ab71c?wsid=/subscriptions/71fa0172-ce90-403c-94a9-14ce1e88f56a/resourcegroups/rg_eastus_44930_1_1677271518341/workspaces/testerinos&tid=82676786-5bc7-43c6-b8f8-b3ee02b0b5f3\nStepRun( Prepare Data ) Status: NotStarted\nStepRun( Prepare Data ) Status: Running\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677275108892
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for run in pipeline_run.get_children():\r\n",
        "    print(run.name, ':')\r\n",
        "    metrics = run.get_metrics()\r\n",
        "    for metric_name in metrics:\r\n",
        "        print('\\t',metric_name, \":\", metrics[metric_name])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677275117433
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "\r\n",
        "# Create a folder for the deployment files\r\n",
        "deployment_folder = './titanic_service'\r\n",
        "os.makedirs(deployment_folder, exist_ok=True)\r\n",
        "print(deployment_folder, 'folder created.')\r\n",
        "\r\n",
        "# Set path for scoring script\r\n",
        "script_file = 'score_titanic.py'\r\n",
        "script_path = os.path.join(deployment_folder,script_file)\r\n",
        "     "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677275184133
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $script_path\r\n",
        "import json\r\n",
        "import joblib\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "\r\n",
        "# Called when the service is loaded\r\n",
        "def init():\r\n",
        "    global model\r\n",
        "    # Get the path to the deployed model file and load it\r\n",
        "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'titanic_model.pkl')\r\n",
        "    model = joblib.load(model_path)\r\n",
        "\r\n",
        "# Called when a request is received\r\n",
        "def run(raw_data):\r\n",
        "    # Get the input data as a numpy array\r\n",
        "    data = np.array(json.loads(raw_data)['data'])\r\n",
        "    # Get a prediction from the model\r\n",
        "    predictions = model.predict(data)\r\n",
        "    # Get the corresponding classname for each prediction (0 or 1)\r\n",
        "    classnames = ['Non-Survived', 'Survived']\r\n",
        "    predicted_classes = []\r\n",
        "    for prediction in predictions:\r\n",
        "        predicted_classes.append(classnames[prediction])\r\n",
        "    # Return the predictions as JSON\r\n",
        "    return json.dumps(predicted_classes)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ws.models['titanic_model']\r\n",
        "print(model.name, 'version', model.version)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677275439528
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\r\n",
        "from azureml.core.model import InferenceConfig\r\n",
        "from azureml.core.webservice import AciWebservice\r\n",
        "from azureml.core import Model\r\n",
        "\r\n",
        "# Configure the scoring environment\r\n",
        "service_env = Environment.get(workspace=ws, name=\"AzureML-sklearn-0.24.1-ubuntu18.04-py37-cpu-inference\")\r\n",
        "service_env.inferencing_stack_version=\"latest\"\r\n",
        "\r\n",
        "inference_config = InferenceConfig(source_directory=deployment_folder,\r\n",
        "                                   entry_script=script_file,\r\n",
        "                                   environment=service_env)\r\n",
        "\r\n",
        "# Configure the web service container\r\n",
        "deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\r\n",
        "\r\n",
        "# Deploy the model as a service\r\n",
        "print('Deploying model...')\r\n",
        "service_name = \"titanic-service\"\r\n",
        "service = Model.deploy(ws, service_name, [model], inference_config, deployment_config, overwrite=True)\r\n",
        "service.wait_for_deployment(True)\r\n",
        "print(service.state)\r\n",
        "     "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677275561352
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for webservice_name in ws.webservices:\r\n",
        "    print(webservice_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677275561941
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Invoke Endpoint\r\n",
        "\r\n",
        "import json\r\n",
        "\r\n",
        "x_new = [[22,1,7.25],[.25,1,.23]]\r\n",
        "print ('Passenger: {}'.format(x_new[0]))\r\n",
        "\r\n",
        "# Convert the array to a serializable list in a JSON document\r\n",
        "input_json = json.dumps({\"data\": x_new})\r\n",
        "\r\n",
        "# Call the web service, passing the input data (the web service will also accept the data in binary format)\r\n",
        "predictions = service.run(input_data = input_json)\r\n",
        "\r\n",
        "# Get the predicted class - it'll be the first (and only) one.\r\n",
        "predicted_classes = json.loads(predictions)\r\n",
        "\r\n",
        "for i in range(len(x_new)):\r\n",
        "    print (\"Passenger {}\".format(x_new[i]), predicted_classes[i] )\r\n",
        "     "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677275664734
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}